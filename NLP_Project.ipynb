{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qdz-Y4MDpj8"
      },
      "source": [
        "1. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ieyf-XhUR2Ws"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet pandas numpy tqdm sentence-transformers faiss-cpu transformers accelerate bitsandbytes ipywidgets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t91EGts6DtyM"
      },
      "source": [
        "2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW7MWjgtSHGU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js1hppV8DyB2"
      },
      "source": [
        "3. Load your CSV and build knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWiEhRt7SNyZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"symptom_precaution.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRlgfpLOD9ph"
      },
      "source": [
        "Build KB:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiiMuGbhSShz"
      },
      "outputs": [],
      "source": [
        "kb_rows = []\n",
        "for _, row in df.iterrows():\n",
        "    disease = str(row[\"Disease\"])\n",
        "    precs = [str(row[c]) for c in [\"Precaution_1\", \"Precaution_2\", \"Precaution_3\", \"Precaution_4\"]]\n",
        "    precs = [p for p in precs if p and p.lower() != \"nan\"]\n",
        "    chunk = f\"Disease: {disease}\\nSelf-care precautions:\\n- \" + \"\\n- \".join(precs)\n",
        "    kb_rows.append({\n",
        "        \"topic\": disease,\n",
        "        \"section\": \"precautions\",\n",
        "        \"chunk\": chunk\n",
        "    })\n",
        "\n",
        "kb_df = pd.DataFrame(kb_rows)\n",
        "kb_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqMdLr73D_ci"
      },
      "source": [
        "4. Embeddings + FAISS index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIsS1X3XSWhe"
      },
      "outputs": [],
      "source": [
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = kb_df[\"chunk\"].tolist()\n",
        "embeddings = embed_model.encode(\n",
        "    texts,\n",
        "    batch_size=32,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ").astype(\"float32\")\n",
        "\n",
        "faiss.normalize_L2(embeddings)\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "print(\"Total indexed vectors:\", index.ntotal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOMP6f_3EGhW"
      },
      "source": [
        "Retriever:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHE0KtJDSa3G"
      },
      "outputs": [],
      "source": [
        "def retrieve(query, k=5):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    scores, indices = index.search(q_emb, k)\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        row = kb_df.iloc[idx]\n",
        "        results.append({\n",
        "            \"score\": float(score),\n",
        "            \"topic\": row[\"topic\"],\n",
        "            \"section\": row[\"section\"],\n",
        "            \"chunk\": row[\"chunk\"]\n",
        "        })\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js2l8Bg3EIAj"
      },
      "source": [
        "Quick test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew0pehVRSjWW"
      },
      "outputs": [],
      "source": [
        "retrieve(\"precautions for dengue\", k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz5Qn6aXELX1"
      },
      "source": [
        "5. Helper to load a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PAPg82aSoAZ"
      },
      "outputs": [],
      "source": [
        "def load_llm(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "    gen = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=160,      # keep small for speed\n",
        "        do_sample=False,         # deterministic\n",
        "        temperature=0.0,\n",
        "        top_p=1.0\n",
        "    )\n",
        "    return tokenizer, gen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Y1lugpEPEK"
      },
      "source": [
        "6. LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YnO-t5UURaK"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Phi-3-mini\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    \"TinyLlama-1.1B\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    \"Qwen-1.8B-Chat\": \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "}\n",
        "\n",
        "llms = {}\n",
        "for name, model_id in models.items():\n",
        "    print(f\"Loading {name} -> {model_id}\")\n",
        "    tokenizer, gen = load_llm(model_id)\n",
        "    llms[name] = {\"tokenizer\": tokenizer, \"generator\": gen}\n",
        "print(\"All models loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2fdMIbnEUdW"
      },
      "source": [
        "7. RAG answer function with safety"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0bZU493UzXK"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are SafeSymp, a careful health assistant. \"\n",
        "    \"Use only the provided context about diseases and precautions. \"\n",
        "    \"Do NOT diagnose diseases. Do NOT prescribe specific medications. \"\n",
        "    \"Give general self-care advice from the context and suggest seeing a doctor \"\n",
        "    \"for serious or persistent symptoms.\"\n",
        ")\n",
        "\n",
        "def rag_answer(query, tokenizer, generator, k=3):\n",
        "    docs = retrieve(query, k=k)\n",
        "    context = \"\\n\\n\".join([d[\"chunk\"] for d in docs])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}\n",
        "    ]\n",
        "\n",
        "    # Build chat-style prompt if supported\n",
        "    try:\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    except Exception:\n",
        "        # Fallback: simple concatenation\n",
        "        prompt = (\n",
        "            SYSTEM_PROMPT + \"\\n\\n\" +\n",
        "            f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "    output = generator(prompt)[0][\"generated_text\"]\n",
        "\n",
        "    # Try to strip the prompt and keep only assistant answer\n",
        "    try:\n",
        "        assistant_start = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"assistant\", \"content\": \"\"}],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        if assistant_start in output:\n",
        "            answer = output.split(assistant_start)[-1].strip()\n",
        "        else:\n",
        "            answer = output[len(prompt):].strip()\n",
        "    except Exception:\n",
        "        answer = output[len(prompt):].strip()\n",
        "\n",
        "    answer = answer.replace(\"<|end|>\", \"\").strip()\n",
        "    return answer, docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWgEdJ0zEbM1"
      },
      "source": [
        "8. Define at least 10 domain questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq7DLuwsW0Q-"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"I have a bad cough and runny nose. What should I do?\",\n",
        "    \"What home precautions are recommended for malaria?\",\n",
        "    \"What self-care steps can I follow for allergy symptoms?\",\n",
        "    \"How can I manage hypothyroidism safely at home?\",\n",
        "    \"What precautions should I follow if I have psoriasis?\",\n",
        "    \"What self-care measures should I take for GERD?\",\n",
        "    \"What precautions are recommended for chronic cholestasis?\",\n",
        "    \"What should I do at home if I have hepatitis A?\",\n",
        "    \"What precautions can help with osteoarthritis pain?\",\n",
        "    \"How can I manage hypoglycemia symptoms at home?\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVbl4yX6EfpL"
      },
      "source": [
        "9. Run evaluation over 3 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbTZVQn6U8tl"
      },
      "outputs": [],
      "source": [
        "import time # Add this line to import the time module\n",
        "\n",
        "results = []\n",
        "\n",
        "# Initialize a dictionary to store all necessary data for batch_results\n",
        "batch_results_detailed = {name: {'responses': [], 'docs': [], 'latencies': [], 'tokens': []} for name in llms.keys()}\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"QUESTION: {q}\\n\")\n",
        "    row = {\"question\": q}\n",
        "    for name, llm in llms.items():\n",
        "        print(f\"--- {name} ---\")\n",
        "        # Capture start time before generation\n",
        "        start_time = time.time()\n",
        "        ans, docs = rag_answer(q, llm[\"tokenizer\"], llm[\"generator\"], k=3)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Token count approximation (simple char count / avg chars per token)\n",
        "        # A more accurate count would use tokenizer.encode(ans) or generator's output\n",
        "        tokens_generated = len(llm[\"tokenizer\"].encode(ans))\n",
        "\n",
        "        print(ans, \"\\n\")\n",
        "        row[name] = ans\n",
        "\n",
        "        # Store detailed results for analysis\n",
        "        batch_results_detailed[name]['responses'].append(ans)\n",
        "        batch_results_detailed[name]['docs'].append([d['chunk'] for d in docs]) # Store only chunks from docs\n",
        "        batch_results_detailed[name]['latencies'].append(end_time - start_time)\n",
        "        batch_results_detailed[name]['tokens'].append(tokens_generated)\n",
        "\n",
        "    results.append(row)\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# After the loop, assign batch_results_detailed to batch_results\n",
        "batch_results = batch_results_detailed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_BmOPIaEpnl"
      },
      "source": [
        "Save for your report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfjOd9iVYzM5"
      },
      "outputs": [],
      "source": [
        "eval_df = pd.DataFrame(results)\n",
        "eval_df.to_csv(\"rag_three_llms_results.csv\", index=False)\n",
        "eval_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtqeZ8l2XVUo"
      },
      "source": [
        "##  Detailed Analysis of Model Differences\n",
        "\n",
        "In this section, we quantitatively and qualitatively analyze the three LLMs based on the batch evaluation results from the 10 domain-specific questions.\n",
        "\n",
        "- **Performance**: Measured by average generation latency (seconds per response) and token efficiency (output tokens per query).\n",
        "- **Accuracy**: Assessed via semantic similarity (cosine similarity using sentence embeddings) between generated responses and retrieved documents, averaged across queries. Higher scores indicate better factual grounding.\n",
        "- **Approach**: Qualitative summary of response styles (e.g., verbosity, structure, hallucination tendency), derived from patterns in outputs. This is semi-automated via keyword analysis for structure (e.g., presence of lists/bullets) and length.\n",
        "\n",
        "We use the stored batch results (assuming a `batch_results` dict from the evaluation loop: `{model_name: {'responses': [list of 10 strs], 'docs': [list of doc lists], 'latencies': [list of 10 floats], 'tokens': [list of 10 ints]}}`). If not already captured, the code below retroactively simulates/computes them if needed.\n",
        "\n",
        "This analysis highlights differences: e.g., smaller models may be faster but less accurate on nuanced symptoms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmzTTKGVXcOu"
      },
      "outputs": [],
      "source": [
        "# Install if needed (in Colab: !pip install sentence-transformers nltk), but assume available\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import time # Import time for latency calculation\n",
        "\n",
        "nltk.download('punkt_tab', quiet=True)  # For sentence tokenization\n",
        "\n",
        "# Load embedding model (lightweight for similarity)\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# batch_results will now be populated by the evaluation loop (cell BbTZVQn6U8tl)\n",
        "# If running this cell independently for testing, a placeholder can be used, but\n",
        "# it's intended to follow the evaluation loop execution.\n",
        "# if 'batch_results' not in globals():\n",
        "#     print(\"Warning: batch_results not found. Please run the evaluation cell first.\")\n",
        "#     # Placeholder for testing if eval cell hasn't run\n",
        "#     batch_results = {name: {\n",
        "#         'responses': [f\"Sample response {i} for {name}\" for i in range(10)],\n",
        "#         'docs': [[f\"Sample doc {i}\"]] * 10, # Simulating context for each response\n",
        "#         'latencies': np.random.uniform(1, 5, 10).tolist(),\n",
        "#         'tokens': np.random.randint(50, 200, 10).tolist()\n",
        "#     } for name in llms.keys()}\n",
        "\n",
        "def compute_accuracy(model_name):\n",
        "    \"\"\"Compute avg cosine similarity between responses and concatenated docs.\"\"\"\n",
        "    data = batch_results[model_name]\n",
        "    scores = []\n",
        "    for resp, doc_list in zip(data['responses'], data['docs']):\n",
        "        if not doc_list or len(doc_list) == 0: # Check if doc_list is empty\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        # Concat docs for simplicity\n",
        "        combined_doc = ' '.join(doc_list)\n",
        "        if len(resp.strip()) == 0 or len(combined_doc.strip()) == 0:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        resp_emb = embedder.encode(resp)\n",
        "        doc_emb = embedder.encode(combined_doc)\n",
        "        scores.append(cosine_similarity([resp_emb], [doc_emb])[0][0])\n",
        "    return np.mean(scores)\n",
        "\n",
        "def analyze_approach(model_name):\n",
        "    \"\"\"Qualitative approach metrics: verbosity (avg sentences), structure (fraction with lists), hallucination proxy (low sim if <0.5).\"\"\"\n",
        "    data = batch_results[model_name]\n",
        "    responses = data['responses']\n",
        "    sim_score = compute_accuracy(model_name)\n",
        "\n",
        "    # Verbosity: average number of sentences\n",
        "    verbosity = np.mean([len(sent_tokenize(resp)) for resp in responses if resp.strip()]) if responses else 0.0\n",
        "\n",
        "    # Structure: fraction of responses containing list/bullet characters\n",
        "    structure_score = np.mean([1 if 'â€¢' in resp or '-' in resp or '*' in resp else 0 for resp in responses])\n",
        "\n",
        "    # Hallucination Tendency: based on sim_score. If sim_score is 0 (due to empty docs), this will be 'High'.\n",
        "    # Note: A score of 0.0 here likely means no grounding context was available during calculation.\n",
        "    hallucination_tendency = 'High' if sim_score < 0.5 else 'Low' if sim_score > 0.7 else 'Medium'\n",
        "\n",
        "    return {\n",
        "        'Avg Sentences': round(verbosity, 1),\n",
        "        'Structure Score (0-1)': round(structure_score, 2),\n",
        "        'Hallucination Tendency': hallucination_tendency\n",
        "    }\n",
        "\n",
        "# Compute metrics for all models\n",
        "metrics = []\n",
        "for name in llms.keys():\n",
        "    data = batch_results[name]\n",
        "    avg_latency = np.mean(data['latencies'])\n",
        "    avg_tokens = np.mean(data['tokens'])\n",
        "    avg_accuracy = compute_accuracy(name)\n",
        "    approach = analyze_approach(name)\n",
        "\n",
        "    metrics.append({\n",
        "        'Model': name,\n",
        "        'Avg Latency (s)': round(avg_latency, 2),\n",
        "        'Avg Tokens': round(avg_tokens),\n",
        "        'Avg Accuracy (Sim)': round(avg_accuracy, 3),\n",
        "        **approach\n",
        "    })\n",
        "\n",
        "# Display comparison table\n",
        "comparison_df = pd.DataFrame(metrics)\n",
        "print(\"### Model Comparison Table\")\n",
        "display(comparison_df)\n",
        "\n",
        "# Optional: Bar plot for key metrics\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics_to_plot = ['Avg Latency (s)', 'Avg Accuracy (Sim)', 'Avg Sentences']\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    axs[i].bar([m['Model'] for m in metrics], [m[metric] for m in metrics])\n",
        "    axs[i].set_title(metric)\n",
        "    axs[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Qualitative Summary (printed)\n",
        "print(\"\\n### Qualitative Approach Differences\")\n",
        "for name in llms.keys():\n",
        "    approach = analyze_approach(name)\n",
        "    print(f\"\\n{name}:\\n  - {approach['Hallucination Tendency']} hallucination (based on doc similarity - **NOTE: Accuracy now reflects whether grounding context was available and used.**).\")\n",
        "    print(f\"  - {'Concise' if approach['Avg Sentences'] < 5 else 'Verbose'} style (avg {approach['Avg Sentences']} sentences/query).\")\n",
        "    print(f\"  - {'Structured' if approach['Structure Score (0-1)'] > 0.5 else 'Narrative'} responses ({approach['Structure Score (0-1)']*100:.0f}% with lists).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMwGwrKgEujx"
      },
      "source": [
        "10. Simple interactive demo UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5GFoZrgY1MB"
      },
      "outputs": [],
      "source": [
        "q_input = widgets.Text(\n",
        "    placeholder=\"Enter a symptom question...\",\n",
        "    description=\"Query:\",\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "out_box = widgets.Output()\n",
        "\n",
        "def on_submit(change):\n",
        "    out_box.clear_output()\n",
        "    with out_box:\n",
        "        q = change[\"new\"].strip()\n",
        "        if not q:\n",
        "            return\n",
        "        print(f\"Query: {q}\\n\")\n",
        "        for name, llm in llms.items():\n",
        "            print(f\"===== {name} =====\")\n",
        "            ans, docs = rag_answer(q, llm[\"tokenizer\"], llm[\"generator\"], k=3)\n",
        "            print(ans, \"\\n\")\n",
        "\n",
        "q_input.observe(on_submit, names=\"value\")\n",
        "display(q_input, out_box)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}